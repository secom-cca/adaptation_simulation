import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent / "src"))

from fastapi import FastAPI, HTTPException, WebSocket, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, Response
from fastapi.security import HTTPBasic, HTTPBasicCredentials
import pandas as pd
import numpy as np
import json
import zipfile
from datetime import datetime
from typing import Dict

from config import DEFAULT_PARAMS, rcp_climate_params, RANK_FILE, ACTION_LOG_FILE, YOUR_NAME_FILE
from models import (
    SimulationRequest, SimulationResponse, CompareRequest, CompareResponse,
    DecisionVar, CurrentValues, BlockRaw
)
from simulation import simulate_simulation
from utils import calculate_scenario_indicators, aggregate_blocks

def _save_results_data(user_name: str, scenario_name: str, block_scores: list):
    """ä¿å­˜ç»“æœæ•°æ®åˆ°æ–‡ä»¶"""
    import pandas as pd
    from pathlib import Path

    data_dir = Path(__file__).parent / "data"
    data_dir.mkdir(exist_ok=True)

    # ä¿å­˜ç”¨æˆ·å
    user_name_file = data_dir / "your_name.csv"
    pd.DataFrame([{"user_name": user_name}]).to_csv(user_name_file, index=False)

    # ä¿å­˜è¯„åˆ†æ•°æ®
    if block_scores:
        df_scores = pd.DataFrame(block_scores)
        df_scores['user_name'] = user_name
        df_scores['scenario_name'] = scenario_name
        df_scores['timestamp'] = pd.Timestamp.utcnow()

        # ä¿å­˜åˆ°block_scores.tsv
        block_scores_file = data_dir / "block_scores.tsv"
        if block_scores_file.exists():
            # è¯»å–ç°æœ‰æ•°æ®
            existing_df = pd.read_csv(block_scores_file, sep='\t')
            # åˆ é™¤åŒä¸€ç”¨æˆ·çš„æ—§æ•°æ®
            existing_df = existing_df[existing_df['user_name'] != user_name]
            # åˆå¹¶æ–°æ•°æ®
            combined_df = pd.concat([existing_df, df_scores], ignore_index=True)
        else:
            combined_df = df_scores

        combined_df.to_csv(block_scores_file, sep='\t', index=False)

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",  # æœ¬åœ°å¼€å‘
        "http://localhost:3001",  # æœ¬åœ°å¼€å‘å¤‡ç”¨ç«¯å£
        "https://climate-adaptation-backend.vercel.app",  # Vercelå‰ç«¯åŸŸå
        "https://climate-adaptation-backend-git-fix-y-axis-adaptation-and-ui-improvements-terryzhang-jp.vercel.app",  # Gitåˆ†æ”¯åŸŸå
        "*"  # ä¸´æ—¶å…è®¸æ‰€æœ‰åŸŸåè¿›è¡Œè°ƒè¯•
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

scenarios_data: Dict[str, pd.DataFrame] = {}

# ç®¡ç†å‘˜è®¤è¯
security = HTTPBasic()

def authenticate_admin(credentials: HTTPBasicCredentials = Depends(security)):
    """éªŒè¯ç®¡ç†å‘˜èº«ä»½"""
    correct_username = "admin"
    correct_password = "climate2025"

    if credentials.username != correct_username or credentials.password != correct_password:
        raise HTTPException(
            status_code=401,
            detail="ç®¡ç†è€…èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ",
            headers={"WWW-Authenticate": "Basic"},
        )
    return credentials.username

@app.get("/ping")
def ping():
    return {"message": "pong"}

@app.post("/simulate", response_model=SimulationResponse)
def run_simulation(req: SimulationRequest):
    scenario_name = req.scenario_name
    mode = req.mode
    decision_df = pd.DataFrame([dv.model_dump() for dv in req.decision_vars]) if req.decision_vars else pd.DataFrame()

    # Update params based on RCP scenario and current values
    params = DEFAULT_PARAMS.copy()
    if req.decision_vars and len(req.decision_vars) > 0:
        rcp_param = rcp_climate_params.get(req.decision_vars[0].cp_climate_params, {})
        params.update(rcp_param)

    all_df = pd.DataFrame()
    block_scores = []

    if mode == "Monte Carlo Simulation Mode":
        # å¹¶è¡ŒåŒ–è’™ç‰¹å¡æ´›ä»¿çœŸä»¥å……åˆ†åˆ©ç”¨å¤šæ ¸CPU
        from concurrent.futures import ProcessPoolExecutor
        import multiprocessing

        def single_simulation(sim_index):
            """å•æ¬¡ä»¿çœŸå‡½æ•°ï¼Œç”¨äºå¹¶è¡Œæ‰§è¡Œ"""
            sim_result = simulate_simulation(
                years=params['years'],
                initial_values=req.current_year_index_seq.model_dump(),
                decision_vars_list=decision_df,
                params=params
            )
            df_sim = pd.DataFrame(sim_result)
            df_sim["Simulation"] = sim_index
            return df_sim

        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨CPUæ ¸å¿ƒè¿›è¡Œå¹¶è¡Œè®¡ç®—
        max_workers = min(multiprocessing.cpu_count(), req.num_simulations)
        print(f"ğŸš€ [Monte Carlo] ä½¿ç”¨ {max_workers} ä¸ªCPUæ ¸å¿ƒå¹¶è¡Œè®¡ç®— {req.num_simulations} æ¬¡ä»¿çœŸ")

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»¿çœŸä»»åŠ¡
            futures = [executor.submit(single_simulation, sim) for sim in range(req.num_simulations)]
            # æ”¶é›†ç»“æœ
            results = [future.result() for future in futures]

        all_df = pd.concat(results, ignore_index=True)
        block_scores = []
        print(f"âœ… [Monte Carlo] å¹¶è¡Œè®¡ç®—å®Œæˆï¼Œå…±å¤„ç† {len(all_df)} è¡Œæ•°æ®")

    elif mode == "Sequential Decision-Making Mode":
        sim_years = np.arange(req.decision_vars[0].year, req.decision_vars[0].year + 1)
        result = simulate_simulation(
            years=sim_years,
            initial_values=req.current_year_index_seq.model_dump(),
            decision_vars_list=decision_df,
            params=params
        )
        all_df = pd.DataFrame(result)
        block_scores = aggregate_blocks(all_df)

        # ãƒ­ã‚°ä¿å­˜
        df_log = pd.DataFrame([dv.model_dump() for dv in req.decision_vars])
        df_log['user_name'] = req.user_name
        df_log['scenario_name'] = scenario_name
        df_log['timestamp'] = pd.Timestamp.utcnow()
        if ACTION_LOG_FILE.exists():
            df_old = pd.read_csv(ACTION_LOG_FILE)
            df_combined = pd.concat([df_old, df_log], ignore_index=True)
        else:
            df_combined = df_log
        df_combined.to_csv(ACTION_LOG_FILE, index=False)

        df_csv = pd.DataFrame(block_scores)
        df_csv['user_name'] = req.user_name
        df_csv['scenario_name'] = scenario_name
        df_csv['timestamp'] = pd.Timestamp.utcnow()
        # ä¿å­˜ç”¨æˆ·åæ–‡ä»¶
        pd.DataFrame([{"user_name": req.user_name}]).to_csv(YOUR_NAME_FILE, index=False)
        if RANK_FILE.exists():
            old = pd.read_csv(RANK_FILE, sep='\t')
            merged = (
                old.set_index(['user_name', 'scenario_name', 'period'])
                .combine_first(df_csv.set_index(['user_name', 'scenario_name', 'period']))
                .reset_index()
            )
            merged.to_csv(RANK_FILE, sep='\t', index=False)
        else:
            df_csv.to_csv(RANK_FILE, sep='\t', index=False)

    
    elif mode == "Predict Simulation Mode":
        # å…¨æœŸé–“ã®äºˆæ¸¬å€¤ã‚’è¨ˆç®—ã™ã‚‹
        params = DEFAULT_PARAMS.copy()
        sim_years = np.arange(req.decision_vars[0].year, params['end_year'] + 1)
        seq_result = simulate_simulation(
            years=sim_years,
            initial_values=req.current_year_index_seq.model_dump(),
            decision_vars_list=decision_df,
            params=params
        )

        all_df = pd.DataFrame(seq_result)
        block_scores = []

    elif mode == "Record Results Mode":
        # å¤„ç†å‰ç«¯å‘é€çš„å®Œæ•´ä»¿çœŸæ•°æ®
        print(f"ğŸ” [Record Results Mode] å¼€å§‹å¤„ç†ç”¨æˆ·: {req.user_name}")
        print(f"ğŸ” [Record Results Mode] æ¥æ”¶åˆ°çš„ä»¿çœŸæ•°æ®é•¿åº¦: {len(req.simulation_data) if req.simulation_data else 0}")

        if req.simulation_data and len(req.simulation_data) > 0:
            print(f"âœ… [Record Results Mode] ä»¿çœŸæ•°æ®æœ‰æ•ˆï¼Œå¼€å§‹å¤„ç†...")
            # å°†ä»¿çœŸæ•°æ®è½¬æ¢ä¸ºDataFrame
            all_df = pd.DataFrame(req.simulation_data)
            print(f"âœ… [Record Results Mode] DataFrameåˆ›å»ºæˆåŠŸï¼Œè¡Œæ•°: {len(all_df)}")

            # è®¡ç®—è¯„åˆ†æ•°æ®
            try:
                block_scores = aggregate_blocks(all_df)
                print(f"âœ… [Record Results Mode] è¯„åˆ†è®¡ç®—æˆåŠŸï¼Œè¯„åˆ†æ•°é‡: {len(block_scores)}")
            except Exception as e:
                print(f"âŒ [Record Results Mode] è¯„åˆ†è®¡ç®—å¤±è´¥: {str(e)}")
                block_scores = []

            # ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
            try:
                print(f"ğŸ’¾ [Record Results Mode] å¼€å§‹ä¿å­˜æ•°æ®...")
                _save_results_data(req.user_name, req.scenario_name, block_scores)
                print(f"âœ… [Record Results Mode] æ•°æ®ä¿å­˜æˆåŠŸ")
            except Exception as e:
                print(f"âŒ [Record Results Mode] æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}")
        else:
            print(f"âš ï¸ [Record Results Mode] æ²¡æœ‰æ¥æ”¶åˆ°æœ‰æ•ˆçš„ä»¿çœŸæ•°æ®")

        # éƒ¨ç½²ç¯å¢ƒä¸‹ä¸éœ€è¦æ–‡ä»¶å¤åˆ¶ï¼Œæ•°æ®é€šè¿‡APIæä¾›
        print(f"âœ… [Record Results Mode] æ•°æ®å·²ä¿å­˜ï¼Œå¯é€šè¿‡APIè®¿é—®")

    else:
        raise HTTPException(status_code=400, detail=f"Unknown mode: {mode}")

    if mode != "Predict Simulation Mode":
        scenarios_data[scenario_name] = all_df.copy()

    return SimulationResponse(
        scenario_name=scenario_name,
        data=all_df.to_dict(orient="records"),
        block_scores=block_scores
    )

@app.get("/ranking")
def get_ranking():
    if not RANK_FILE.exists():
        return []
    df = pd.read_csv(RANK_FILE, sep='\t')
    latest = df.sort_values('timestamp').drop_duplicates(['user_name', 'scenario_name', 'period'], keep='last')
    rank_df = (
        latest.groupby('user_name')['total_score']
        .mean()
        .reset_index()
        .sort_values('total_score', ascending=False)
        .reset_index(drop=True)
    )
    rank_df['rank'] = rank_df.index + 1
    return rank_df.to_dict(orient='records')

@app.post("/compare", response_model=CompareResponse)
def compare_scenario_data(req: CompareRequest):
    selected_data = {name: scenarios_data[name] for name in req.scenario_names if name in scenarios_data}
    if not selected_data:
        raise HTTPException(status_code=404, detail="No scenarios found for given names.")
    indicators_result = {name: calculate_scenario_indicators(df) for name, df in selected_data.items()}
    return CompareResponse(message="Comparison results", comparison=indicators_result)

@app.get("/scenarios")
def list_scenarios():
    return {"scenarios": list(scenarios_data.keys())}

@app.get("/export/{scenario_name}")
def export_scenario_data(scenario_name: str):
    if scenario_name not in scenarios_data:
        raise HTTPException(status_code=404, detail="Scenario not found.")
    return scenarios_data[scenario_name].to_csv(index=False)

@app.get("/block_scores")
def get_block_scores():
    if not RANK_FILE.exists():
        return []
    try:
        df = pd.read_csv(RANK_FILE, sep="\t")
        return df.to_dict(orient="records")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/user_data/{user_name}")
def get_user_data(user_name: str):
    """è·å–æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰æ•°æ®ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§"""
    try:
        print(f"ğŸ” [API] è·å–ç”¨æˆ·æ•°æ®: {user_name}")

        result = {
            "user_name": user_name,
            "your_name_csv": f"user_name\n{user_name}",
            "decision_log_csv": "",
            "block_scores_tsv": "",
            "found": False,
            "data_complete": False,
            "periods_found": 0
        }

        # è·å–å†³ç­–æ—¥å¿—
        if ACTION_LOG_FILE.exists():
            df_log = pd.read_csv(ACTION_LOG_FILE)
            user_logs = df_log[df_log['user_name'] == user_name]
            if not user_logs.empty:
                result["decision_log_csv"] = user_logs.to_csv(index=False)
                result["found"] = True
                print(f"âœ… [API] æ‰¾åˆ°å†³ç­–æ—¥å¿—: {len(user_logs)} æ¡è®°å½•")

        # è·å–è¯„åˆ†æ•°æ®å¹¶éªŒè¯å®Œæ•´æ€§
        if RANK_FILE.exists():
            df_scores = pd.read_csv(RANK_FILE, sep='\t')
            user_scores = df_scores[df_scores['user_name'] == user_name]
            if not user_scores.empty:
                # æ£€æŸ¥æ˜¯å¦æœ‰3ä¸ªæ—¶æœŸçš„æ•°æ®
                periods = user_scores['period'].unique()
                expected_periods = ['2026-2050', '2051-2075', '2076-2100']

                result["periods_found"] = len(periods)
                result["data_complete"] = len(periods) >= 3

                if result["data_complete"]:
                    # æŒ‰æ—¶æœŸæ’åºï¼Œç¡®ä¿é¡ºåºæ­£ç¡®
                    user_scores_sorted = user_scores.sort_values('period')
                    result["block_scores_tsv"] = user_scores_sorted.to_csv(sep='\t', index=False)
                    print(f"âœ… [API] æ‰¾åˆ°å®Œæ•´è¯„åˆ†æ•°æ®: {len(periods)} ä¸ªæ—¶æœŸ")
                else:
                    result["block_scores_tsv"] = user_scores.to_csv(sep='\t', index=False)
                    print(f"âš ï¸ [API] è¯„åˆ†æ•°æ®ä¸å®Œæ•´: åªæœ‰ {len(periods)} ä¸ªæ—¶æœŸ")

                result["found"] = True

        if not result["found"]:
            print(f"âŒ [API] æœªæ‰¾åˆ°ç”¨æˆ·æ•°æ®: {user_name}")
            raise HTTPException(status_code=404, detail=f"No data found for user: {user_name}")

        if not result["data_complete"]:
            print(f"âš ï¸ [API] ç”¨æˆ·æ•°æ®ä¸å®Œæ•´: {user_name}, æ—¶æœŸæ•°: {result['periods_found']}")

        return result

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ [API] è·å–ç”¨æˆ·æ•°æ®å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/debug/file_status")
def get_file_status():
    """æ£€æŸ¥æ‰€æœ‰å¿…éœ€æ–‡ä»¶çš„çŠ¶æ€"""
    from pathlib import Path

    data_dir = Path(__file__).parent / "data"
    frontend_data_dir = Path(__file__).parent.parent / "frontend" / "public" / "results" / "data"

    files_to_check = [
        ("your_name.csv", YOUR_NAME_FILE),
        ("decision_log.csv", ACTION_LOG_FILE),
        ("block_scores.tsv", RANK_FILE)
    ]

    status = {
        "backend_data_dir": str(data_dir),
        "frontend_data_dir": str(frontend_data_dir),
        "backend_files": {},
        "frontend_files": {},
        "summary": {
            "all_backend_files_exist": True,
            "all_frontend_files_exist": True,
            "missing_files": []
        }
    }

    # æ£€æŸ¥åç«¯æ–‡ä»¶
    for file_name, file_path in files_to_check:
        exists = file_path.exists()
        size = file_path.stat().st_size if exists else 0
        status["backend_files"][file_name] = {
            "exists": exists,
            "path": str(file_path),
            "size": size
        }
        if not exists:
            status["summary"]["all_backend_files_exist"] = False
            status["summary"]["missing_files"].append(f"backend/{file_name}")

    # æ£€æŸ¥å‰ç«¯æ–‡ä»¶
    for file_name, _ in files_to_check:
        frontend_file = frontend_data_dir / file_name
        exists = frontend_file.exists()
        size = frontend_file.stat().st_size if exists else 0
        status["frontend_files"][file_name] = {
            "exists": exists,
            "path": str(frontend_file),
            "size": size
        }
        if not exists:
            status["summary"]["all_frontend_files_exist"] = False
            status["summary"]["missing_files"].append(f"frontend/{file_name}")

    return status


# ã‚µãƒ¼ãƒã«é€ä¿¡ã•ã‚Œã¦ã„ã‚‹ãƒ­ã‚°ã‚’WebSocketã§å—ä¿¡ã€‚ç¾åœ¨ã¯backendã«ä¿å­˜ä¸­
@app.websocket("/ws/log")
async def websocket_log_endpoint(websocket: WebSocket):
    await websocket.accept()
    log_path = Path(__file__).parent / "data" / "user_log.jsonl"
    while True:
        try:
            data = await websocket.receive_text()
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(data + "\n")
        except Exception as e:
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ‡æ–­ãªã©ã§ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰çµ‚äº†
            break

# æ‰¹é‡æ¥æ”¶å‰ç«¯logæ•°æ®çš„APIç«¯ç‚¹
@app.post("/logs/batch")
async def receive_batch_logs(request: dict):
    """æ‰¹é‡æ¥æ”¶å‰ç«¯logæ•°æ®"""
    try:
        logs = request.get("logs", [])
        if not logs:
            return {"status": "success", "message": "No logs to process"}

        log_path = Path(__file__).parent / "data" / "user_log.jsonl"

        # ç¡®ä¿dataç›®å½•å­˜åœ¨
        log_path.parent.mkdir(exist_ok=True)

        # æ‰¹é‡å†™å…¥logæ•°æ®
        with open(log_path, "a", encoding="utf-8") as f:
            for log_entry in logs:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

        print(f"âœ… [API] æ‰¹é‡æ¥æ”¶ {len(logs)} æ¡logæ•°æ®")
        return {
            "status": "success",
            "message": f"Successfully received {len(logs)} logs",
            "count": len(logs)
        }

    except Exception as e:
        print(f"âŒ [API] æ‰¹é‡logæ¥æ”¶å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to save logs: {str(e)}")

# ç»“æŸå®éªŒAPIç«¯ç‚¹
@app.post("/experiment/end")
async def end_experiment(request: dict):
    """å¤„ç†å®éªŒç»“æŸè¯·æ±‚"""
    try:
        user_name = request.get("user_name")
        logs = request.get("logs", [])

        if not user_name:
            raise HTTPException(status_code=400, detail="User name is required")

        log_path = Path(__file__).parent / "data" / "user_log.jsonl"
        log_path.parent.mkdir(exist_ok=True)

        # å†™å…¥ç»“æŸå®éªŒçš„æ—¥å¿—
        end_log = {
            "type": "ExperimentEnd",
            "user_name": user_name,
            "timestamp": datetime.now().isoformat(),
            "total_logs": len(logs)
        }

        with open(log_path, "a", encoding="utf-8") as f:
            # å†™å…¥æ‰€æœ‰ç”¨æˆ·è¡Œä¸ºæ—¥å¿—
            for log_entry in logs:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            # å†™å…¥å®éªŒç»“æŸæ ‡è®°
            f.write(json.dumps(end_log, ensure_ascii=False) + "\n")

        print(f"âœ… [Experiment End] ç”¨æˆ· {user_name} å®éªŒç»“æŸï¼Œä¿å­˜ {len(logs)} æ¡æ—¥å¿—")

        return {
            "status": "success",
            "message": f"Experiment ended for user {user_name}",
            "logs_saved": len(logs)
        }

    except Exception as e:
        print(f"âŒ [Experiment End] å¤„ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to end experiment: {str(e)}")

# è·å–ç”¨æˆ·æ—¥å¿—APIç«¯ç‚¹
@app.get("/user-logs/{user_name}")
async def get_user_logs(user_name: str):
    """è·å–æŒ‡å®šç”¨æˆ·çš„æ‰€æœ‰æ—¥å¿—æ•°æ®"""
    try:
        log_path = Path(__file__).parent / "data" / "user_log.jsonl"

        if not log_path.exists():
            return {"logs": [], "message": "No logs found"}

        user_logs = []
        with open(log_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    try:
                        log = json.loads(line.strip())
                        if log.get('user_name') == user_name:
                            user_logs.append(log)
                    except json.JSONDecodeError:
                        continue

        print(f"âœ… [User Logs] è·å–ç”¨æˆ· {user_name} çš„æ—¥å¿—: {len(user_logs)} æ¡")

        return {
            "user_name": user_name,
            "logs": user_logs,
            "total_count": len(user_logs)
        }

    except Exception as e:
        print(f"âŒ [User Logs] è·å–å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get user logs: {str(e)}")

# ç®¡ç†å‘˜è·¯ç”±
@app.get("/admin/dashboard")
async def get_admin_dashboard(admin: str = Depends(authenticate_admin)):
    """è·å–ç®¡ç†å‘˜ä»ªè¡¨æ¿æ•°æ®"""
    try:
        data_dir = Path(__file__).parent / "data"

        # è¯»å–ç”¨æˆ·æ—¥å¿—
        user_log_file = data_dir / "user_log.jsonl"
        user_logs = []
        if user_log_file.exists():
            with open(user_log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        user_logs.append(json.loads(line.strip()))

        # è¯»å–è¯„åˆ†æ•°æ®
        block_scores = []
        if RANK_FILE.exists():
            df = pd.read_csv(RANK_FILE, sep='\t')
            block_scores = df.to_dict('records')

        # ç»Ÿè®¡ä¿¡æ¯
        unique_users = set()
        for log in user_logs:
            if 'user_name' in log:
                unique_users.add(log['user_name'])

        # æŒ‰ç”¨æˆ·åˆ†ç»„çš„è¯„åˆ†æ•°æ®
        user_scores = {}
        for score in block_scores:
            user_name = score['user_name']
            if user_name not in user_scores:
                user_scores[user_name] = []
            user_scores[user_name].append(score)

        # æœ€è¿‘æ´»åŠ¨
        recent_logs = sorted(user_logs, key=lambda x: x.get('timestamp', ''), reverse=True)[:50]

        return {
            "summary": {
                "total_users": len(unique_users),
                "total_logs": len(user_logs),
                "total_simulations": len(block_scores),
                "last_activity": recent_logs[0]['timestamp'] if recent_logs else None
            },
            "users": list(unique_users),
            "user_scores": user_scores,
            "recent_activity": recent_logs,
            "data_files": {
                "user_log_size": user_log_file.stat().st_size if user_log_file.exists() else 0,
                "block_scores_size": RANK_FILE.stat().st_size if RANK_FILE.exists() else 0
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

@app.get("/admin/data-files")
async def list_data_files(admin: str = Depends(authenticate_admin)):
    """è·å–dataæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶çš„åˆ—è¡¨å’Œä¿¡æ¯"""
    try:
        data_dir = Path(__file__).parent / "data"
        files_info = []

        if data_dir.exists():
            for file_path in data_dir.iterdir():
                if file_path.is_file():
                    stat = file_path.stat()
                    files_info.append({
                        "name": file_path.name,
                        "size": stat.st_size,
                        "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "type": file_path.suffix.lower()
                    })

        return {"files": files_info}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

@app.get("/admin/preview-file/{filename}")
async def preview_file_content(filename: str, admin: str = Depends(authenticate_admin)):
    """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã«å–å¾—"""
    try:
        data_dir = Path(__file__).parent / "data"
        file_path = data_dir / filename

        print(f"Preview request for file: {filename}")
        print(f"File path: {file_path}")
        print(f"File exists: {file_path.exists()}")

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
        if not file_path.resolve().is_relative_to(data_dir.resolve()):
            raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã§ã™")

        if not file_path.exists():
            raise HTTPException(status_code=404, detail="ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        file_extension = file_path.suffix.lower()
        print(f"File extension: {file_extension}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆå¤§ãã™ãã‚‹å ´åˆã¯åˆ¶é™ï¼‰
        max_size = 5 * 1024 * 1024  # 5MB
        file_size = file_path.stat().st_size
        print(f"File size: {file_size}")

        if file_size > max_size:
            return {
                "filename": filename,
                "type": "error",
                "message": "ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ï¼ˆ5MBä»¥ä¸Šï¼‰",
                "data": []
            }

        try:
            if file_extension in ['.csv']:
                # CSV ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                print(f"Processing CSV file: {filename}")
                try:
                    df = pd.read_csv(file_path, encoding='utf-8')
                except UnicodeDecodeError:
                    df = pd.read_csv(file_path, encoding='shift_jis')

                print(f"CSV loaded successfully. Shape: {df.shape}")
                print(f"Columns: {df.columns.tolist()}")

                return {
                    "filename": filename,
                    "type": "table",
                    "columns": df.columns.tolist(),
                    "data": df.head(100).fillna('').to_dict('records'),  # æœ€åˆã®100è¡Œã®ã¿ã€NaNã‚’ç©ºæ–‡å­—ã«
                    "total_rows": len(df)
                }

            elif file_extension in ['.tsv']:
                # TSV ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                print(f"Processing TSV file: {filename}")
                try:
                    df = pd.read_csv(file_path, sep='\t', encoding='utf-8')
                except UnicodeDecodeError:
                    df = pd.read_csv(file_path, sep='\t', encoding='shift_jis')

                print(f"TSV loaded successfully. Shape: {df.shape}")
                print(f"Columns: {df.columns.tolist()}")

                return {
                    "filename": filename,
                    "type": "table",
                    "columns": df.columns.tolist(),
                    "data": df.head(100).fillna('').to_dict('records'),
                    "total_rows": len(df)
                }

            elif file_extension in ['.jsonl']:
                # JSONL ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                import json
                lines = []
                with open(file_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i >= 100:  # æœ€åˆã®100è¡Œã®ã¿
                            break
                        if line.strip():
                            try:
                                lines.append(json.loads(line.strip()))
                            except json.JSONDecodeError:
                                continue

                # ç·è¡Œæ•°ã‚’å–å¾—
                total_lines = sum(1 for line in open(file_path, 'r', encoding='utf-8') if line.strip())

                return {
                    "filename": filename,
                    "type": "json",
                    "data": lines,
                    "total_rows": total_lines
                }

            else:
                # ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read(10000)  # æœ€åˆã®10KB

                return {
                    "filename": filename,
                    "type": "text",
                    "data": content,
                    "total_size": file_path.stat().st_size
                }

        except Exception as parse_error:
            # ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¡¨ç¤º
            print(f"Parse error for {filename}: {str(parse_error)}")
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(10000)
            except Exception as read_error:
                print(f"Read error for {filename}: {str(read_error)}")
                content = f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(read_error)}"

            return {
                "filename": filename,
                "type": "text",
                "data": content,
                "error": f"è§£æã‚¨ãƒ©ãƒ¼: {str(parse_error)}"
            }

    except HTTPException:
        raise
    except Exception as e:
        print(f"General error for {filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

@app.get("/admin/download/file/{filename}")
async def download_single_file(filename: str, admin: str = Depends(authenticate_admin)):
    """æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        data_dir = Path(__file__).parent / "data"
        file_path = data_dir / filename

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ï¼šãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒã‚’é˜²ã
        if not file_path.resolve().is_relative_to(data_dir.resolve()):
            raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã§ã™")

        if not file_path.exists():
            raise HTTPException(status_code=404, detail="ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        return FileResponse(
            path=file_path,
            filename=filename,
            media_type="application/octet-stream"
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

@app.get("/admin/download/all")
async def download_all_data(admin: str = Depends(authenticate_admin)):
    """ä¸‹è½½æ‰€æœ‰æ•°æ®çš„å‹ç¼©åŒ…"""
    try:
        data_dir = Path(__file__).parent / "data"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        zip_filename = f"climate_simulation_data_{timestamp}.zip"
        zip_path = data_dir / zip_filename

        # åˆ›å»ºå‹ç¼©åŒ…
        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            # æ·»åŠ æ‰€æœ‰æ•°æ®æ–‡ä»¶
            for file_path in data_dir.glob("*.jsonl"):
                zipf.write(file_path, file_path.name)
            for file_path in data_dir.glob("*.tsv"):
                zipf.write(file_path, file_path.name)
            for file_path in data_dir.glob("*.csv"):
                zipf.write(file_path, file_path.name)

        return FileResponse(
            path=zip_path,
            filename=zip_filename,
            media_type="application/zip",
            headers={"Content-Disposition": f"attachment; filename={zip_filename}"}
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

@app.get("/admin/download/logs")
async def download_user_logs(admin: str = Depends(authenticate_admin)):
    """ä¸‹è½½ç”¨æˆ·æ—¥å¿—æ–‡ä»¶"""
    try:
        data_dir = Path(__file__).parent / "data"
        log_file = data_dir / "user_log.jsonl"

        if not log_file.exists():
            raise HTTPException(status_code=404, detail="ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

        return FileResponse(
            path=log_file,
            filename=f"user_logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl",
            media_type="application/json"
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ãƒ­ã‚°ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

@app.get("/admin/download/scores")
async def download_scores(admin: str = Depends(authenticate_admin)):
    """ä¸‹è½½è¯„åˆ†æ•°æ®æ–‡ä»¶"""
    try:
        if not RANK_FILE.exists():
            raise HTTPException(status_code=404, detail="è©•ä¾¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

        return FileResponse(
            path=RANK_FILE,
            filename=f"block_scores_{datetime.now().strftime('%Y%m%d_%H%M%S')}.tsv",
            media_type="text/tab-separated-values"
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

@app.get("/admin/data-stats")
async def get_data_stats(admin: str = Depends(authenticate_admin)):
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºæ¸…ç©ºå‰ç¡®è®¤"""
    try:
        data_dir = Path(__file__).parent / "data"

        # ç»Ÿè®¡ç”¨æˆ·æ—¥å¿—
        user_log_file = data_dir / "user_log.jsonl"
        user_logs = []
        unique_users = set()

        if user_log_file.exists():
            with open(user_log_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            log = json.loads(line.strip())
                            user_logs.append(log)
                            if 'user_name' in log:
                                unique_users.add(log['user_name'])
                        except json.JSONDecodeError:
                            continue

        # ç»Ÿè®¡è¯„åˆ†æ•°æ®
        block_scores = []
        simulation_periods = set()

        if RANK_FILE.exists():
            df = pd.read_csv(RANK_FILE, sep='\t')
            block_scores = df.to_dict('records')
            simulation_periods = set(df['period'].unique()) if 'period' in df.columns else set()

        # ç»Ÿè®¡å†³ç­–æ—¥å¿—
        decision_logs = []
        if ACTION_LOG_FILE.exists():
            df_log = pd.read_csv(ACTION_LOG_FILE)
            decision_logs = df_log.to_dict('records')

        # è®¡ç®—æ–‡ä»¶å¤§å°
        file_sizes = {}
        data_files = [
            ("user_log.jsonl", user_log_file),
            ("block_scores.tsv", RANK_FILE),
            ("decision_log.csv", ACTION_LOG_FILE),
            ("your_name.csv", YOUR_NAME_FILE)
        ]

        total_size = 0
        for file_name, file_path in data_files:
            if file_path.exists():
                size = file_path.stat().st_size
                file_sizes[file_name] = {
                    "size_bytes": size,
                    "size_mb": round(size / (1024 * 1024), 2),
                    "exists": True
                }
                total_size += size
            else:
                file_sizes[file_name] = {
                    "size_bytes": 0,
                    "size_mb": 0,
                    "exists": False
                }

        # è·å–æœ€æ—©å’Œæœ€æ–°çš„æ´»åŠ¨æ—¶é—´
        earliest_activity = None
        latest_activity = None

        if user_logs:
            timestamps = [log.get('timestamp') for log in user_logs if log.get('timestamp')]
            if timestamps:
                earliest_activity = min(timestamps)
                latest_activity = max(timestamps)

        stats = {
            "summary": {
                "total_users": len(unique_users),
                "total_logs": len(user_logs),
                "total_simulations": len(block_scores),
                "total_decision_logs": len(decision_logs),
                "simulation_periods": len(simulation_periods),
                "earliest_activity": earliest_activity,
                "latest_activity": latest_activity,
                "total_size_mb": round(total_size / (1024 * 1024), 2)
            },
            "files": file_sizes,
            "users": list(unique_users),
            "periods": list(simulation_periods)
        }

        return stats

    except Exception as e:
        print(f"âŒ [Admin] æ•°æ®ç»Ÿè®¡è·å–å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

@app.post("/admin/clear-data")
async def clear_all_data(admin: str = Depends(authenticate_admin)):
    """æ¸…ç©ºæ‰€æœ‰æ•°æ®æ–‡ä»¶å†…å®¹ï¼ˆä¿ç•™æ–‡ä»¶ä½†æ¸…ç©ºå†…å®¹ï¼‰"""
    try:
        data_dir = Path(__file__).parent / "data"

        # è·å–æ¸…ç©ºå‰çš„ç»Ÿè®¡ä¿¡æ¯
        stats_before = await get_data_stats(admin)

        # å®šä¹‰éœ€è¦æ¸…ç©ºçš„æ–‡ä»¶
        files_to_clear = [
            ("user_log.jsonl", data_dir / "user_log.jsonl"),
            ("block_scores.tsv", RANK_FILE),
            ("decision_log.csv", ACTION_LOG_FILE),
            ("your_name.csv", YOUR_NAME_FILE)
        ]

        cleared_files = []
        errors = []

        # æ¸…ç©ºæ¯ä¸ªæ–‡ä»¶çš„å†…å®¹
        for file_name, file_path in files_to_clear:
            try:
                if file_path.exists():
                    # è·å–æ–‡ä»¶åŸå§‹å¤§å°
                    original_size = file_path.stat().st_size

                    # æ¸…ç©ºæ–‡ä»¶å†…å®¹ä½†ä¿ç•™æ–‡ä»¶
                    with open(file_path, 'w', encoding='utf-8') as f:
                        # å¯¹äºTSVå’ŒCSVæ–‡ä»¶ï¼Œä¿ç•™è¡¨å¤´
                        if file_name == "block_scores.tsv":
                            f.write("user_name\tscenario_name\tperiod\ttotal_score\ttimestamp\n")
                        elif file_name == "decision_log.csv":
                            f.write("year,planting_trees_amount,house_migration_amount,dam_levee_construction_cost,paddy_dam_construction_cost,capacity_building_cost,transportation_invest,agricultural_RnD_cost,cp_climate_params,user_name,scenario_name,timestamp\n")
                        elif file_name == "your_name.csv":
                            f.write("user_name\n")
                        # user_log.jsonl å®Œå…¨æ¸…ç©º

                    cleared_files.append({
                        "file": file_name,
                        "original_size_bytes": original_size,
                        "original_size_mb": round(original_size / (1024 * 1024), 2),
                        "status": "cleared"
                    })
                    print(f"âœ… [Admin] å·²æ¸…ç©ºæ–‡ä»¶: {file_name} (åŸå¤§å°: {original_size} bytes)")
                else:
                    cleared_files.append({
                        "file": file_name,
                        "original_size_bytes": 0,
                        "original_size_mb": 0,
                        "status": "not_existed"
                    })
                    print(f"â„¹ï¸ [Admin] æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_name}")

            except Exception as file_error:
                error_msg = f"æ–‡ä»¶ {file_name} æ¸…ç©ºå¤±è´¥: {str(file_error)}"
                errors.append(error_msg)
                print(f"âŒ [Admin] {error_msg}")

        # æ¸…ç©ºå†…å­˜ä¸­çš„scenarios_data
        global scenarios_data
        scenarios_data.clear()
        print("âœ… [Admin] å·²æ¸…ç©ºå†…å­˜ä¸­çš„scenarios_data")

        # å‡†å¤‡å“åº”
        result = {
            "success": len(errors) == 0,
            "message": "ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢ãŒå®Œäº†ã—ã¾ã—ãŸ" if len(errors) == 0 else f"ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {len(errors)}ä»¶",
            "stats_before": stats_before["summary"],
            "cleared_files": cleared_files,
            "errors": errors,
            "timestamp": datetime.now().isoformat(),
            "total_files_processed": len(files_to_clear),
            "successful_clears": len(cleared_files) - len(errors)
        }

        print(f"ğŸ§¹ [Admin] æ•°æ®æ¸…ç©ºæ“ä½œå®Œæˆ: æˆåŠŸ {result['successful_clears']}/{result['total_files_processed']} ä¸ªæ–‡ä»¶")

        return result

    except Exception as e:
        print(f"âŒ [Admin] æ•°æ®æ¸…ç©ºæ“ä½œå¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
